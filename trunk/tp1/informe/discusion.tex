\begin{section}{Discusión}
	En esta sección, buscaremos conclusiones a la información suministrada por los gráficos de la sección anterior.
	
	El primer gráfico presenta una primer aproximación al error relativo de los tres algoritmos (Figura:\ref{fig:51p}).
	Observamos que a medida que la cantidad de términos de las series aumenta, el error cometido en el cálculo de $\pi$ de cada una de ellas disminuye, utilizando una cantidad de dígitos igual a 51. Las conclusiones tomadas son particulares a este gráficos, y sin más información son imposibles de generalizar.
	
	El error introducido al aproximar $\pi$ con la serie de $Gregory$ es logarítmico (en la escala del gráfico) en función de la cantidad de términos de la serie calculados. Si la escala en el eje $y$ fuese lineal, entonces obtendríamos que el error de la serie de $Gregory$ decrece linealmente conforme aumenta la cantidad de iteraciones.
	
	En la primer aproximación a la comprensión del error relativo cometido al aproximar $\pi$ con la fórmula de $Machin$ en función de la cantidad de iteraciones, vemos que los valores obtenidos en cada iteración forman una recta con pendiente negativa. Como la escala utilizada para representar dicho error es la logarítmica, podemos decir que el error decrece exponencialmente.
	
	Además observamos que a partir de la décima iteración el gráfico no muestra de forma clara lo que ocurre con el error en la fórmula de $Machin$. Suponemos que se debe al hecho de que los valores que representan a $Machin$ se superponen con los valores correspondientes a $Ramanujan$.
	
	Por otro lado, podemos apreciar que con pocas iteraciones la serie de $Ramanujan$ obtiene un error constante, similar observación puede hacerse a $Machin$ a partir de la iteración diez.\\
	
	Los siguientes cuatro gráficos (Figura \ref{fig:gregory_1000it}, \ref{fig:ramanujan_42it} \ref{fig:machin_100it} y \ref{fig:machin_10it}) profundizan el análisis, centrándose ahora en cada serie en particular, ya que al tener errores relativos de diferentes órdenes de magnitud, el análisis de las series en un mismo gráfico puede conducir a errores de interpretación propios de utilizar una escala que pueda contemplar todas las curvas.
	
	La diferencia en la cantidad de iteraciones con las que fueron realizados los gráficos depende de la velocidad en la que cada serie reduce el error relativo a $\pi$. Si bien con esta información sóla no alcanza para poder plantear una función de convergencia para cada una, nos da una idea de cuál de ellas se acerca más rápido a obtener un error tan pequeño como se desee (suponiendo para esta conclusión la ausencia de error en los cálculos y aritmética tan grande como uno quiera).
	
	El primer gráfico corresponde a la serie de Gregory, analizándola en mil iteraciones, utilizando precisiones de 5, 15, 25 y 45 bits. 
	
	Es interesante ver que dada cualquier precisión el error oscila substancialmente entre cada iteración, siendo más notorio utilizando precisión de 15 bits. Esto sucede ya que el orden de magnitud del error relativo varia entre las iteraciones pares e impares. Además, es interesante resaltar que la serie suma en las iteraciones pares y resta en las impares, produciendo al menos en las primeras iteraciones que se aleje o acerque al $\pi$ teórico por arriba en las impares, y por abajo en las pares.
	
	La curva correspondiente a la presición de cinco bits muestra que a partir de la iteración 50 (aproximadamente) el error deja de cambiar, se estabiliza. Que no pueda mejorar, es decir que consiga mantener un error estable, significa que el valor que aporta (suma o resta) en cada iteración es un número suficientemente chico para que al truncar este valor sea desechado. El error en el cálculo de las operaciones también entraría en el mismo caso y por consiguiente, no vemos que la curva varie a medida que la cantidad de iteraciones aumenta.
	
	Cabe destacar que en las primeras iteraciones, hablando nuevamente de precisión de cinco bits, el valor calculado por la serie de Gregory, posee menos error que al estabilizarse. Esta observación se desprende necesariamente de que en cada iteración el valor oscila y como resultado de esto en las primeras iteraciones se encuentra más cercano al $\pi$ teórico. A razón de lo recién mencionado, podemos concluir que al usar una precisión muy pequeña, es conveniente aproximar al valor exacto con pocas iteraciones.
	
	Este comportamiento también sucede aunque en menor medida, utilizando precisión de quince dígitos. Si bien no alcanzan los valores que toma el eje $x$ para ver si se estabiliza el error, todo parece indicar que sí, ya que cada vez la pendiente positiva se hace menos pronunciada. Llama mucho la atención como entre las 150 y 250 iteraciones se obtienen errores relativos mucho más chicos que utilizando mayor precisión, nuevamente este comportamiento viene de la mano de la naturaleza de la serie (sumar en las iteraciones pares y restar en las impares valores que impactan en el resultado final). Para precisiones de alrededor de 15 bits, es conveniente entonces utilizar entre 100 y 300 iteraciones.

	A medida que aumenta la precisión la diferencia entre cada par de curvas disminuye, como ejemplo de esto podemos notar a las dos que utilizan precisiones de 25 y 45 bits. Estas curvas disminuyen suavemente, minimizando el error en cada iteración, su pendiente es cada vez menor, nuevamente llevándonos a la conclusión que eventualmente se estabilizará en el error relativo dado por las condiciones previamente mencionadas (diferencia en la precisión del $\pi$ considerado como exacto y el aproximado, errores arrastrados por la presición finita y velocidad de aproximación). Sin duda, a medida que la presición es mayor, el error relativo disminuye. Podemos realizar, si se posee tanto tiempo como recursos, más iteraciones si contamos con una aritmética finita más grande (como es el caso de las dos últimas curvas analizadas).\\

	
	Continuando con el siguiente gráfico, Ramanujan, vemos que el error relativo depende estrictamente de la precisión utilizada. Recién con precisión de 45 bits, existe diferencia entre la primera y segunda iteración. Esto habla fuertemente de la velocidad con la que se aproxima dicho algoritmo. Es interesante observar, relacionando la información con la del gráfico anterior (Gregory), cuando la precisión es muy chica, en este caso 5 bits, el error relativo cometido es ligeramente mayor utilizando el segundo algoritmo. A medida que la precisión aumenta, esta relación se invierte.\\
	
	En resultados, se incluyeron dos gráficos más bajo el mismo tipo de análisis para Machin, la diferencia entre ellos rádica únicamente en los valores que toma el eje $x$, siendo los mismos datos de entrada utilizados.
	
	El primero de los dos gráficos muestra simplemente que a mayor precisión, menor error relativo. El segundo en cambio, muestra que con poca precisión es mejor realizar muy pocas iteraciones, ya que el error aumenta.\\
	\texttt{LO SIGUIENTE NO ME CIERRA!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!}\\
	Y si extendemos esta precisión, no importa la precisión, el mejor valor (es decir menor error) se obtiene en la iteración cuatro, donde posteriormente empeoran. Valiendo los mismos detalles mencionados para las series anteriores sobre el error, vemos aquí que nuevamente alejan la solución de la real.\\

	Como otro resultado, tenemos al gráfico que muestra el error relativo con respecto a la cantidad de dígitos, fijando la cantidad de iteraciones en $42$ (Figura:\ref{fig:42it})
	
	La apreciación (lineal/exponencial para nuestro caso) del decrecimiento de los errores en función de la cantidad de dígitos es la misma que en función de la cantidad de iteraciones para $Ramanujan$ y $Machin$. El primero de los dos en este caso, decrece lineal (pensándolo en escala logarítmica) sin cambiar el valor de su pendiente a partir de cierto valor. Parece reducir entonces, de manera exponencial el error cometido conforme aumenta la precisión utilizada (debido a la relación entre la escala y la forma de la pendiente).
	
	$Gregory$ muestra una pendiente logarítmica monótona decreciente hasta un $t$ cercano a doce, donde según este gráfico, se estabiliza y se transforma en constante. Si esto sucede, significaría que no tiene sentido seguir calculando más iteraciones, ya que el error de estos dígitos no van a mejorar con respecto al valor exacto de la constante a calcular. Nuevamente, esto viene de la mano de la propagación de errores de cálculo.
	
	La Figura:\ref{fig:5it} fija la cantidad de iteraciones en un valor menor (cinco en este caso), para analizar si existe diferencias con respecto al anterior de 42.
	
	Vemos que con poca precisión los errores siguen siendo similares.
	
	%$Machin$ consigue mejores aproximaciones que $Gregory$ fijando una cantidad de iteraciones, cuando la precisión es baja pierde mayor información al truncar lo que se ve reflejado en la diferencia de errores al aumentar dicha presición (necesita más dígitos para estabilizarse). No olvidar que la serie de $Gregory$ tarda más iteraciones que el resto en reducir su error relativo contra el $\pi$ exacto, esto, es otro causante de esta diferencia (analizada en los gráficos de iteraciones en función del error relativo).
	
	En el primer y segundo algoritmo, $Gregory$ y $Machin$, podemos inferir que existe una relación entre la cantidad de iteraciones de la serie y la precisión utilizada. La única diferencia entre los dos gráficos es la cantidad de iteraciones, y podemos observar que si la cantidad de iteraciones es 42, el error relativo disminuye en ambos (se estabiliza en $Gregory$ y parece continuar decreciendo en $Machin$). En cambio, si la cantidad de iteraciones es 5, la serie de $Machin$ estabiliza su error a partir de la precisión 30 y el error de $Gregory$ aumenta. \\
	
	Los gráficos \ref{fig:gregory_51p}, \ref{fig:ramanujan_51p} y \ref{fig:machin_51p} son muy interesantes, ya que muestran que elegir la máxima precisión no siempre nos lleva como resultado obtener un error más pequeño. Existe como fue mencionado en el párrafo pasado, una relación estricta entre cantidad de iteraciones y precisión. El ejemplo más claro, es el gráfico de $Gregory$. Realizando tan sólo cinco iteraciones del algoritmo, obtenemos mejores resultados que las otras iteraciones cuando la cantidad de dígitos es chica. Esto también sucede en $Machin$, las diferencias son menores, pero aún percibibles.
	
	Para el caso de $Ramanujan$, no es posible ver que realizar menos iteraciones cuando la precisión es chica mejore la solución. Lo interesante que tiene este gráfico es que tampoco es mejorada por aumentar la cantidad de iteraciones. Quizás esta diferencia se haga visible si la cantidad de iteraciones aumenta significativamente. Lamentablemente, no podemos hacer este análisis debido a que nuestra implementación no nos permite excedernos de 42 iteraciones.
	
	Los tres gráficos muestran que cuanto mayor es la precisión y mayor la cantidad de iteraciones, mejores soluciones vamos a obtener.
	
	Si bien los estos algoritmos necesitan de realizar la sumatoria infinita para converger a la constante ($\pi$), dado un valor fijo para el cálculo de la sumatoria y/o bits fijos, su mejor aproximación vendrá dada por $Ramanujan$, seguido de $Machin$ y luego $Gregory$.
	\\
	
	En el último gráfico adjuntado, Figura:\ref{fig:cotas} vemos que una cierta cantidad de dígitos, el análisis teórico deja de servir como cota. Esto sucede cuando los errores cometidos se 'estabilizan', es decir tanto en el cálculo como en la graficación de los mismos existe un error imposible de desechar.
	
	Pensamos que el hecho de que en un momento la cota queda por debajo de los valores se debe (además de las razones anteriores) a que sólo calculamos tres términos de las series y la cota esta expresada en función a la cantidad de dígitos utilizados.
	
	Por ese motivo podemos ver que $Ramanujan$ se mantiene siempre por debajo de la cota, es decir, lo hace porque a medida que trabaja con una mayor precisión logra disminuir su error.
\end{section}
